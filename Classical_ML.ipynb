{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EORBrpfgPCh-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31015214-5b16-4253-a026-abce39d9af8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: urlextract in /usr/local/lib/python3.7/dist-packages (1.6.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.7/dist-packages (from urlextract) (2.10)\n",
            "Requirement already satisfied: uritools in /usr/local/lib/python3.7/dist-packages (from urlextract) (4.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from urlextract) (3.7.1)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.7/dist-packages (from urlextract) (2.5.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: empath in /usr/local/lib/python3.7/dist-packages (0.89)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from empath) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->empath) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->empath) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->empath) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->empath) (2.10)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# !pip install joblib\n",
        "# !pip install threadpoolctl\n",
        "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV, SGDClassifier\n",
        "from sklearn.linear_model import RidgeClassifier, RidgeClassifierCV, Perceptron, PassiveAggressiveClassifier\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier, VotingClassifier, StackingClassifier, BaggingClassifier \n",
        "from sklearn.neighbors import KNeighborsClassifier, RadiusNeighborsClassifier, NearestCentroid\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
        "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB, CategoricalNB, ComplementNB\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.multioutput import ClassifierChain, MultiOutputClassifier\n",
        "from sklearn.semi_supervised import LabelPropagation, LabelSpreading, SelfTrainingClassifier\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer, TfidfTransformer\n",
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.base import BaseEstimator, TransformerMixin, ClassifierMixin\n",
        "\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "\n",
        "from collections import Counter\n",
        "from scipy.sparse import csr_matrix\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "import re\n",
        "\n",
        "!pip install urlextract\n",
        "import urlextract\n",
        "find_urls = urlextract.URLExtract().find_urls\n",
        "\n",
        "!pip install empath\n",
        "from empath import Empath\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "stem = nltk.PorterStemmer().stem\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "path = '/Users/hashem/Python/TDI/capstone/data/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ozm_FlRQbd5F"
      },
      "outputs": [],
      "source": [
        "class get_text(BaseEstimator, TransformerMixin):\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "    def transform(self, X):\n",
        "        return X.text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZg7md1WNGHc"
      },
      "outputs": [],
      "source": [
        "# to merge two-part texts (text + selftext in submissions)\n",
        "def merge_text(t1, t2):\n",
        "    if isinstance(type(t1), str) and t1!='[removed]':\n",
        "        if isinstance(type(t2),str) and t2!='[removed]':\n",
        "            text = t1+' '+t2\n",
        "        else:\n",
        "            text = t1\n",
        "    elif isinstance(type(t2),str) and t2 != '[removed]':\n",
        "        text = t2\n",
        "    else:\n",
        "        text = ''\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qNY_UYznOhvg"
      },
      "outputs": [],
      "source": [
        "# make texts lower-case\n",
        "class lower_case(BaseEstimator, TransformerMixin):\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        X_transformed = X['text'].apply(str.lower)\n",
        "        return pd.DataFrame(np.c_[X['author'], X_transformed], columns = ['author', 'text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKZ6SD-rOy0y"
      },
      "outputs": [],
      "source": [
        "# replace all urls with 'URL'\n",
        "class replace_urls(BaseEstimator, TransformerMixin):\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "    \n",
        "    def transform(self, X, y=None):\n",
        "        X_transformed = []\n",
        "        for text in X['text']:\n",
        "            urls = list(set(find_urls(text)))\n",
        "            urls.sort(key=lambda x: len(x), reverse=True)\n",
        "            for url in urls:\n",
        "                text = text.replace(url, 'URL')\n",
        "            X_transformed.append(text)\n",
        "\n",
        "        return pd.DataFrame(np.c_[X['author'], X_transformed], columns = ['author', 'text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LR1Ps9Vbpkf0"
      },
      "outputs": [],
      "source": [
        "# replace all numbers with 'NUM'\n",
        "class replace_numbers(BaseEstimator, TransformerMixin):\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        X_transformed = []\n",
        "        for text in X['text']:\n",
        "            text_tr = re.sub(r'\\d+(?:\\.\\d*)?(?:[eE][+-]?\\d+)?', 'NUM', text)\n",
        "            X_transformed.append(text_tr)\n",
        "\n",
        "        return pd.DataFrame(np.c_[X['author'], X_transformed], columns = ['author', 'text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hXP4NSJQpsh1"
      },
      "outputs": [],
      "source": [
        "# limit size of the text\n",
        "class limit_size(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, size):\n",
        "        self.size = size\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        index = [len(text) < self.size for text in X['text']]\n",
        "        return X[index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPA5u6Fvpx7f"
      },
      "outputs": [],
      "source": [
        "# remove punctuations\n",
        "class remove_punctuation(BaseEstimator, TransformerMixin):\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        X_transformed = []\n",
        "        for text in X['text']:\n",
        "            text_tr = re.sub(r'\\W+', ' ', text, flags=re.M)\n",
        "            X_transformed.append(text_tr)\n",
        "\n",
        "        return pd.DataFrame(np.c_[X['author'], X_transformed], columns = ['author', 'text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2h2qOK_8p5s9"
      },
      "outputs": [],
      "source": [
        "# remove key words\n",
        "class remove_key_words(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, remove_list):\n",
        "        self.remove_list = remove_list\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "    \n",
        "    def transform(self, X, y=None):\n",
        "        X_transformed = []\n",
        "        for text in X['text']:\n",
        "            for word in self.remove_list:\n",
        "                text = text.replace(word, '')\n",
        "            X_transformed.append(text)\n",
        "\n",
        "        return pd.DataFrame(np.c_[X['author'], X_transformed], columns = ['author', 'text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EhLKBc5Gp_cJ"
      },
      "outputs": [],
      "source": [
        "# remove stopwords\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "class remove_stopwords(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, stopwords=stopwords):\n",
        "        self.stopwords = stopwords\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        X_transformed = []\n",
        "        for text in X['text']:\n",
        "            text_=''\n",
        "            for word in nltk.tokenize.word_tokenize(text):\n",
        "                if word not in self.stopwords:\n",
        "                    text_ = ' '.join([text_,word])\n",
        "            X_transformed.append(text_)\n",
        "\n",
        "        return pd.DataFrame(np.c_[X['author'], X_transformed], columns = ['author', 'text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WRZFCE8xqF4h"
      },
      "outputs": [],
      "source": [
        "# counting the number of words in a text\n",
        "class wordCount(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, stemming=True):\n",
        "        self.stemming = stemming\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "    \n",
        "    def transform(self, X, y=None):\n",
        "        X_transformed = []\n",
        "        for text in X['text']:\n",
        "            word_counts = Counter(text.split())\n",
        "            \n",
        "            if self.stemming:\n",
        "                stemmed_word_counts = Counter()\n",
        "                for word, count in word_counts.items():\n",
        "                    stemmed_word = stem(word)\n",
        "                    stemmed_word_counts[stemmed_word] += count\n",
        "                word_counts = stemmed_word_counts\n",
        "                \n",
        "            X_transformed.append(word_counts)\n",
        "            \n",
        "        return pd.DataFrame(np.c_[X['author'], X_transformed], columns = ['author', 'word_counts'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6naQLr5aqWxa"
      },
      "outputs": [],
      "source": [
        "# transfering words to vectors\n",
        "class toVector(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, vocabulary_size=10000):\n",
        "        self.vocabulary_size = vocabulary_size\n",
        "        \n",
        "    def fit(self, X, y=None):\n",
        "        total_count = Counter()\n",
        "        for word_count in X['word_counts']:\n",
        "            for word, count in word_count.items():\n",
        "                # total_count[word] += count\n",
        "                total_count[word] += min(count, 10)\n",
        "        most_common = total_count.most_common()[:self.vocabulary_size]\n",
        "        self.vocabulary_ = {word: index + 1 for index, (word, count) in enumerate(most_common)}\n",
        "        return self\n",
        "    \n",
        "    def transform(self, X, y=None):\n",
        "        rows = []\n",
        "        cols = []\n",
        "        data = []\n",
        "        for row, word_count in enumerate(X['word_counts']):\n",
        "            for word, count in word_count.items():\n",
        "                rows.append(row)\n",
        "                cols.append(self.vocabulary_.get(word, 0))\n",
        "                data.append(count)\n",
        "        csr = csr_matrix((data, (rows, cols)), shape=(len(X), len(self.vocabulary_) + 1))\n",
        "        df = pd.DataFrame.sparse.from_spmatrix(csr)\n",
        "        df.insert(0,'author', X['author'])\n",
        "        return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSBmcivLqh8U"
      },
      "outputs": [],
      "source": [
        "class empath(BaseEstimator, TransformerMixin):\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        empath = []\n",
        "        for text in X['text']:\n",
        "            empath.append(Empath().analyze(text, normalize=True))\n",
        "        df = pd.DataFrame(empath)\n",
        "        df.insert(0, 'author', X['author'])\n",
        "        return df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from random import shuffle\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import HashingVectorizer, TfidfVectorizer, TfidfTransformer\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "path = '/content/drive/MyDrive/data_23/'\n",
        "\n",
        "groups = {'general': ['MentalHealthSupport','mentalhealth','mental','personalitydisorders',\n",
        "                      'mentalillness','MentalHealthPH'],\n",
        "          \n",
        "          'control': ['askscience','askscience2','LifeProTips','LifeProTips2','AskReddit','AskReddit2',\n",
        "                      'answers','answers2', 'AskScienceFiction','AskScienceFiction2','TrueAskReddit',\n",
        "                      'TrueAskReddit2'],\n",
        "\n",
        "          'adhd': ['ADHD','ADHD2'],\n",
        "\n",
        "          'autism': ['aspergaers','autism2','AutisticQueers','AutismInWomen','Aspergers_Elders',\n",
        "                     'asperger','AutisticPride','autism','AutismTranslated','aspergers_dating',\n",
        "                     'aspergirls','AutisticAdults'],\n",
        "          \n",
        "          'anxiety': ['anxiety'],\n",
        "\n",
        "          'ocd': ['OCD'],\n",
        "\n",
        "          'ptsd': ['ptsd','CPTSD'],\n",
        "\n",
        "          'phobia': ['Phobia','emetophobia','Agoraphobia'],\n",
        "\n",
        "          'socialanxiety':['socialanxiety','socialanxiety2'],\n",
        "          \n",
        "          'depression': ['depression1','depression2','depression3'],\n",
        "\n",
        "          'sadness': ['sad11','sad22','sad33'],\n",
        "          \n",
        "          'bipolar': ['bipolar','BipolarReddit'],\n",
        "          \n",
        "          'schizophrenia': ['schizophrenia','paranoidschizophrenia','schizoaffective','Psychosis'],\n",
        "\n",
        "          'cluster_a': ['Schizoid','Schizotypal','ParanoidPersonality',\n",
        "                        'Paranoid','ParanoiaCheck','Paranoia'],\n",
        "          'cluster_b': ['BorderlinePDisorder','BPD','Borderline','hpd','NPD','narcissism',\n",
        "                        'sociopath', 'psychopath','Psychopathy','aspd'],\n",
        "          'cluster_c': ['OCPD','AvPD','Avoidant', 'DPD'],\n",
        "\n",
        "          'selfharm': ['selfharm','StopSelfHarm','AdultSelfHarm',\n",
        "                       'SuicideWatch11','SuicideWatch22','SuicideWatch33'],\n",
        "          \n",
        "          'addiction': ['addiction','alcoholism'],\n",
        "\n",
        "          'eating': ['ARFID', 'bulimia','eating_disorders','EDAnonymous','EatingDisorders'],\n",
        "\n",
        "          'dpdr': ['dpdr'],\n",
        "          'dysmorphic': ['DysmorphicDisorder', 'BodyAcceptance'],\n",
        "          'tourettes': ['Tourettes'],\n",
        "          'anger': ['Anger'],\n",
        "          }\n"
      ],
      "metadata": {
        "id": "1V4c0bsth-C3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v = pickle.load(open('/content/drive/MyDrive/model/w2v.pkl','rb'))"
      ],
      "metadata": {
        "id": "_XKud80va_L5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize(data, maxlen=100, embedding_dim=100):\n",
        "    \"\"\"\n",
        "    Tokenizes reviews, truncates the number of tokens if more than `maxlen`, \n",
        "    and vectorizes each token. Returns a three-dimensional array of shape\n",
        "    n reviews x `maxlen` x `embedding_dim`. \n",
        "    \"\"\"\n",
        "    # Create empty array\n",
        "    vectorized_data = np.zeros(shape=(len(data), maxlen, embedding_dim))\n",
        "    \n",
        "    for row, case in enumerate(data):\n",
        "        # Preprocess each review\n",
        "        tokens = simple_preprocess(case)\n",
        "        \n",
        "        # Truncate long reviews\n",
        "        if len(tokens) > maxlen:\n",
        "            tokens = tokens[:maxlen]\n",
        "        \n",
        "        # Get vector for each token in review\n",
        "        for col, token in enumerate(tokens):\n",
        "            try:\n",
        "                word_vector = w2v[token]\n",
        "                # Add vector to array\n",
        "                vectorized_data[row, col] = word_vector[:embedding_dim]\n",
        "            except KeyError:\n",
        "                pass\n",
        "    \n",
        "    return vectorized_data.reshape(len(data), -1)"
      ],
      "metadata": {
        "id": "eHYYWoFwHNV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf = SGDClassifier()\n",
        "classes = list(groups.keys())\n",
        "files = os.listdir(path)\n",
        "for f in files[:10]:\n",
        "    df = pd.read_csv(path+f)\n",
        "    df = df.dropna()\n",
        "    while len(df)>1000:\n",
        "        df_ = df.iloc[:1000]\n",
        "        x_b = df_.x.values\n",
        "        y_b = df_.y.values\n",
        "        x_tr = vectorize(x_b)\n",
        "        clf.partial_fit(x_tr, y_b, classes=classes)\n",
        "        df = df.iloc[1000:]\n",
        "    clf.partial_fit(x_tr, y_b, classes=classes)\n",
        "\n",
        "f = files[10]\n",
        "df= pd.read_csv(path+f)\n",
        "while len(df)>1000:\n",
        "    df_ = df.iloc[:1000]\n",
        "    x_test = df_.x.values\n",
        "    y_test = df_.y.values\n",
        "    x_test_tr = vectorize(x_test)\n",
        "    y_pred = clf.predict(x_test_tr)\n",
        "    print(accuracy_score(y_test,y_pred))\n",
        "    df = df.iloc[1000:]\n"
      ],
      "metadata": {
        "id": "irnUn1gpGUxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for f in os.listdir(path)[:10]:\n",
        "    df = pd.read_csv(path+f, lineterminator='\\n', engine='c')\n",
        "    x_tr = w2v.wv()"
      ],
      "metadata": {
        "id": "k0PoOMmtFjn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(columns=['x','y'])\n",
        "i=0\n",
        "for f in os.scandir(path):\n",
        "    df_ = pd.read_csv(f, lineterminator='\\n', engine='c')\n",
        "    df_ = df_[[len(i)>500 for i in df_.x]]\n",
        "    df = pd.concat([df,df_], ignore_index=True)\n",
        "    i+=1\n",
        "    if i==200: break\n",
        "len(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1EUOjRTEpoC9",
        "outputId": "16d73984-e1bc-4d43-95e3-6ed550967b55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1802816"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(df.x.values,df.y.values, test_size=.2)\n",
        "len(x_train), len(x_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lkdb_uZnuMtT",
        "outputId": "e25ca677-697e-42ce-b1f6-6ede3ba727a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1442252, 360564)"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = Pipeline([('vec', TfidfVectorizer()),\n",
        "                 ('clf', MultinomialNB(alpha=.01))])\n",
        "pipe.fit(x_train, y_train)\n",
        "y_pred = pipe.predict(x_test)\n",
        "pickle.dump(pipe,open('/content/drive/MyDrive/model/pipe_N200_L500_NBalpha01.pkl', 'wb'))\n",
        "accuracy_score(y_test,y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0LhNHoypoLX",
        "outputId": "9a97200c-1523-4a04-f7bd-2ce204d670e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.25356940792758015"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from random import shuffle\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import HashingVectorizer, TfidfVectorizer, TfidfTransformer\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "path = '/content/drive/MyDrive/data_23/'\n",
        "\n",
        "groups = {'general': ['MentalHealthSupport','mentalhealth','mental','personalitydisorders',\n",
        "                      'mentalillness','MentalHealthPH'],\n",
        "          \n",
        "          'control': ['askscience','askscience2','LifeProTips','LifeProTips2','AskReddit','AskReddit2',\n",
        "                      'answers','answers2', 'AskScienceFiction','AskScienceFiction2','TrueAskReddit',\n",
        "                      'TrueAskReddit2'],\n",
        "\n",
        "          'adhd': ['ADHD','ADHD2'],\n",
        "\n",
        "          'autism': ['aspergaers','autism2','AutisticQueers','AutismInWomen','Aspergers_Elders',\n",
        "                     'asperger','AutisticPride','autism','AutismTranslated','aspergers_dating',\n",
        "                     'aspergirls','AutisticAdults'],\n",
        "          \n",
        "          'anxiety': ['anxiety'],\n",
        "\n",
        "          'ocd': ['OCD'],\n",
        "\n",
        "          'ptsd': ['ptsd','CPTSD'],\n",
        "\n",
        "          'phobia': ['Phobia','emetophobia','Agoraphobia'],\n",
        "\n",
        "          'socialanxiety':['socialanxiety','socialanxiety2'],\n",
        "          \n",
        "          'depression': ['depression1','depression2','depression3'],\n",
        "\n",
        "          'sadness': ['sad11','sad22','sad33'],\n",
        "          \n",
        "          'bipolar': ['bipolar','BipolarReddit'],\n",
        "          \n",
        "          'schizophrenia': ['schizophrenia','paranoidschizophrenia','schizoaffective','Psychosis'],\n",
        "\n",
        "          'cluster_a': ['Schizoid','Schizotypal','ParanoidPersonality',\n",
        "                        'Paranoid','ParanoiaCheck','Paranoia'],\n",
        "          'cluster_b': ['BorderlinePDisorder','BPD','Borderline','hpd','NPD','narcissism',\n",
        "                        'sociopath', 'psychopath','Psychopathy','aspd'],\n",
        "          'cluster_c': ['OCPD','AvPD','Avoidant', 'DPD'],\n",
        "\n",
        "          'selfharm': ['selfharm','StopSelfHarm','AdultSelfHarm',\n",
        "                       'SuicideWatch11','SuicideWatch22','SuicideWatch33'],\n",
        "          \n",
        "          'addiction': ['addiction','alcoholism'],\n",
        "\n",
        "          'eating': ['ARFID', 'bulimia','eating_disorders','EDAnonymous','EatingDisorders'],\n",
        "\n",
        "          'dpdr': ['dpdr'],\n",
        "          'dysmorphic': ['DysmorphicDisorder', 'BodyAcceptance'],\n",
        "          'tourettes': ['Tourettes'],\n",
        "          'anger': ['Anger'],\n",
        "          }\n"
      ],
      "metadata": {
        "id": "vEOpVqq9m2uo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(columns=['x','y'])\n",
        "i=0\n",
        "for f in os.scandir(path):\n",
        "    df_ = pd.read_csv(f, lineterminator='\\n', engine='c')\n",
        "    df_ = df_[[len(i)>500 for i in df_.x]]\n",
        "    df = pd.concat([df,df_], ignore_index=True)\n",
        "    i+=1\n",
        "    if i==250: break\n",
        "x_train, x_test, y_train, y_test = train_test_split(df.x.values,df.y.values, test_size=.2)\n",
        "len(x_train), len(x_test)\n",
        "pipe = Pipeline([('vec', TfidfVectorizer()),\n",
        "                 ('clf', MultinomialNB(alpha=.01))])\n",
        "pipe.fit(x_train, y_train)\n",
        "y_pred = pipe.predict(x_test)\n",
        "pickle.dump(pipe,open('/content/drive/MyDrive/model/pipe_N250_L500_NBalpha01.pkl', 'wb'))\n",
        "accuracy_score(y_test,y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mx1J7AQMglQa",
        "outputId": "005fd718-c9d6-4a0d-fe16-ebc9e3c5577d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.25008498810166574"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "df = pd.DataFrame(columns=['x','y'])\n",
        "i=0\n",
        "for f in os.scandir(path):\n",
        "    df_ = pd.read_csv(f, lineterminator='\\n', engine='c')\n",
        "    df_ = df_[[len(i)>500 for i in df_.x]]\n",
        "    df = pd.concat([df,df_], ignore_index=True)\n",
        "    i+=1\n",
        "    if i==50: break\n",
        "x_train, x_test, y_train, y_test = train_test_split(df.x.values,df.y.values, test_size=.2)\n",
        "len(x_train), len(x_test)\n",
        "pipe = Pipeline([('vec', TfidfVectorizer()),\n",
        "                 ('clf', RandomForestClassifier())])\n",
        "pipe.fit(x_train, y_train)\n",
        "y_pred = pipe.predict(x_test)\n",
        "\n",
        "accuracy_score(y_test,y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCJKXVrMqbhZ",
        "outputId": "a515df59-9eb7-4395-b54a-f1eb50ffe9c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.1908152054493518"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(columns=['x','y'])\n",
        "i=0\n",
        "for f in os.scandir(path):\n",
        "    df_ = pd.read_csv(f, lineterminator='\\n', engine='c')\n",
        "    df_ = df_[[len(i)>500 for i in df_.x]]\n",
        "    df = pd.concat([df,df_], ignore_index=True)\n",
        "    i+=1\n",
        "    if i==400: break\n",
        "x_train, x_test, y_train, y_test = train_test_split(df.x.values,df.y.values, test_size=.2)\n",
        "len(x_train), len(x_test)\n",
        "pipe = Pipeline([('vec', TfidfVectorizer()),\n",
        "                 ('clf', MultinomialNB(alpha=.01))])\n",
        "pipe.fit(x_train, y_train)\n",
        "y_pred = pipe.predict(x_test)\n",
        "accuracy_score(y_test,y_pred)"
      ],
      "metadata": {
        "id": "3mXcXLggqXjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(columns=['x','y'])\n",
        "i=0\n",
        "for f in os.scandir(path):\n",
        "    df_ = pd.read_csv(f, lineterminator='\\n', engine='c')\n",
        "    df_ = df_[[len(i)>500 for i in df_.x]]\n",
        "    df = pd.concat([df,df_], ignore_index=True)\n",
        "    i+=1\n",
        "    if i==200: break\n",
        "x_train, x_test, y_train, y_test = train_test_split(df.x.values,df.y.values, test_size=.2)\n",
        "len(x_train), len(x_test)\n",
        "pipe = Pipeline([('hash', HashingVectorizer(alternate_sign=False)),\n",
        "                 ('tfidf', TfidfTransformer()),\n",
        "                 ('clf', MultinomialNB(alpha=.01))])\n",
        "pipe.fit(x_train, y_train)\n",
        "y_pred = pipe.predict(x_test)\n",
        "accuracy_score(y_test,y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xLJYoRihJc-",
        "outputId": "7defe3be-223f-40d0-8d87-068a84b0f4a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.2467467634040004"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(columns=['x','y'])\n",
        "i=0\n",
        "for f in os.scandir(path):\n",
        "    df_ = pd.read_csv(f, lineterminator='\\n', engine='c')\n",
        "    df_ = df_[[len(i)>500 for i in df_.x]]\n",
        "    df = pd.concat([df,df_], ignore_index=True)\n",
        "    i+=1\n",
        "    if i==400: break\n",
        "x_train, x_test, y_train, y_test = train_test_split(df.x.values,df.y.values, test_size=.2)\n",
        "len(x_train), len(x_test)\n",
        "pipe = Pipeline([('hash', HashingVectorizer(alternate_sign=False)),\n",
        "                 ('tfidf', TfidfTransformer()),\n",
        "                 ('clf', MultinomialNB(alpha=.01))])\n",
        "pipe.fit(x_train, y_train)\n",
        "y_pred = pipe.predict(x_test)\n",
        "accuracy_score(y_test,y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QguTibYIs2RC",
        "outputId": "52cab753-4e9b-4452-c5bd-f05556954ba1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.23536583358932725"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i=0\n",
        "for g in groups:\n",
        "    files=[]\n",
        "    for folder in groups[g]:\n",
        "        files.extend(os.scandir(path+folder))\n",
        "    c=0\n",
        "    shuffle(files)\n",
        "    for f in files:\n",
        "        c+=1\n",
        "        if c==17: break\n",
        "        df = pd.read_csv(f, usecols=['text'], lineterminator='\\n', engine='c')\n",
        "        idx=[len(x)>150 for x in df.text]\n",
        "        txt = df.text[idx]\n",
        "        if i:\n",
        "            X = np.concatenate([X, txt])\n",
        "            y = np.concatenate([y,[g]*len(txt)])\n",
        "        else:\n",
        "            X = np.array(txt)\n",
        "            y = np.array([g]*len(txt))\n",
        "            i+=1\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=.15)\n",
        "vec = HashingVectorizer()\n",
        "X_train_tr = vec.fit_transform(X_train)\n",
        "X_test_tr = vec.transform(X_test)\n",
        "\n",
        "pickle.dump(vec, open('/content/drive/MyDrive/model/vec.pkl', 'wb'))\n",
        "pickle.dump(X_train_tr, open('/content/drive/MyDrive/model/X_train_tr.pkl', 'wb'))\n",
        "pickle.dump(X_test_tr, open('/content/drive/MyDrive/model/X_test_tr.pkl', 'wb'))\n",
        "pickle.dump(y_train, open('/content/drive/MyDrive/model/y_train.pkl', 'wb'))\n",
        "pickle.dump(y_test, open('/content/drive/MyDrive/model/y_test.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "AF6zEu6hpoSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "import pickle\n",
        "\n",
        "# vec = pickle.load(open('/content/drive/MyDrive/model/vec.pkl', 'rb'))\n",
        "X_train_tr = pickle.load(open('/content/drive/MyDrive/model/X_train_tr.pkl', 'rb'))\n",
        "X_test_tr = pickle.load(open('/content/drive/MyDrive/model/X_test_tr.pkl', 'rb'))\n",
        "y_train = pickle.load(open('/content/drive/MyDrive/model/y_train.pkl', 'rb'))\n",
        "y_test = pickle.load(open('/content/drive/MyDrive/model/y_test.pkl', 'rb'))"
      ],
      "metadata": {
        "id": "fZ8UklDgF33h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf = OneVsRestClassifier(SGDClassifier(), n_jobs=-1)\n",
        "\n",
        "i = 0\n",
        "k=['general','control','adhd','autism','anxiety','depression','sadness','bipolar','schizophrenia','anger',\n",
        " 'cluster_a','cluster_b','cluster_c','selfharm','addiction','eating','dpdr','dysmorphic','tourettes']\n",
        "while i <= len(y_train)//1000:\n",
        "    clf.partial_fit(X_train_tr[i:i+1000], y_train[i:i+1000], classes=k)\n",
        "    pickle.dump(clf, open('/content/drive/MyDrive/model/clf.pkl', 'wb'))\n",
        "    pickle.dump(i, open('/content/drive/MyDrive/model/last_i.pkl', 'wb'))\n",
        "    i += 1000\n",
        "\n",
        "y_pred = clf.predict(X_test_tr)\n",
        "print('Accuracy: {:.2f}%\\n'.format(100*accuracy_score(y_test, y_pred)))"
      ],
      "metadata": {
        "id": "dii4o8Dcn5sf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_tr[i+3:i+4]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ti5nyB2o1AQ",
        "outputId": "40371630-7261-4a6e-fc8f-32c24d5b4ced"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<1x1048576 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 29 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNU3s1ffbd5S"
      },
      "outputs": [],
      "source": [
        "vec = TfidfVectorizer()\n",
        "X_train_tr = vec.fit_transform(X_train)\n",
        "X_test_tr  = vec.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qz5XWkGtbd5S",
        "outputId": "284b4e3f-7fe8-4db4-c5ba-a88e1d78e859"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB(alpha=0.001)"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ],
      "source": [
        "clf = MultinomialNB(alpha=.01)\n",
        "clf.fit(X_train_tr,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W0X4RIsWbd5T"
      },
      "outputs": [],
      "source": [
        "y_pred = clf.predict(X_test_tr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1AKpuJMPbd5T",
        "outputId": "b1ddb633-fa99-4369-8252-889932ba151b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 21.66%\n",
            "\n",
            "Precision (micro): 21.66%\n",
            "Recall (micro): 21.66%\n",
            "F1 (micro): 21.66%\n",
            "\n",
            "Precision (macro): 23.48%\n",
            "Recall (macro): 22.77%\n",
            "F1 (macro): 22.56%\n"
          ]
        }
      ],
      "source": [
        "print('Accuracy: {:.2f}%\\n'.format(100*accuracy_score(y_test, y_pred)))\n",
        "print('Precision (micro): {:.2f}%'.format(100*precision_score(y_test, y_pred, average='micro'))) \n",
        "print('Recall (micro): {:.2f}%'.format(100*recall_score(y_test, y_pred, average='micro')))  \n",
        "print('F1 (micro): {:.2f}%\\n'.format(100*f1_score(y_test, y_pred, average='micro'))) \n",
        "print('Precision (macro): {:.2f}%'.format(100*precision_score(y_test, y_pred, average='macro'))) \n",
        "print('Recall (macro): {:.2f}%'.format(100*recall_score(y_test, y_pred, average='macro'))) \n",
        "print('F1 (macro): {:.2f}%'.format(100*f1_score(y_test, y_pred, average='macro')))   "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pickle.dump(clf, open(path+'clf.pkl', 'wb'))\n",
        "pickle.dump(vec, open(path+'vec.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "mTwEBTsf2A5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xx = [\"Wikipedia has received praise for its enablement of the democratization of knowledge, extent of coverage, unique structure, culture, and reduced degree of commercial bias; but criticism for exhibiting systemic bias, particularly gender bias against women and alleged ideological bias.[13][14] The reliability of Wikipedia was frequently criticized in the 2000s but has improved over time, as Wikipedia has been generally praised in the late 2010s and early 2020s.[3][13][15] The website's coverage of controversial topics such as American politics and major events like the COVID-19 pandemic has received substantial media attention. It has been censored by world governments, ranging from specific pages to the entire site. Nevertheless, Wikipedia has become an element of popular culture, with references in books, films, and academic studies. In April 2018, Facebook and YouTube announced that they would help users detect fake news by suggesting fact-checking links to related Wikipedia articles.[16][17] Articles on breaking news are often accessed as a source of frequently updated information about those events\"]\n",
        "xx_tr = vec.transform(xx)\n",
        "clf.predict_proba(xx_tr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdozZI6Nm5Ht",
        "outputId": "7df20a04-b052-4dd0-b9c0-b458c0fc2d9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3.81853967e-10, 3.62916963e-05, 2.85033089e-10, 8.00661799e-18,\n",
              "        1.26523161e-07, 8.90305929e-14, 5.77042579e-02, 1.22176247e-03,\n",
              "        1.14104877e-09, 4.06144758e-15, 1.21280732e-10, 2.74370014e-44,\n",
              "        9.41037549e-01, 9.31560767e-10, 4.73358119e-14, 9.73188602e-09,\n",
              "        1.71139763e-23]])"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels = list(clf.classes_)\n",
        "pred = list(clf.predict_proba(xx_tr)[0])"
      ],
      "metadata": {
        "id": "U3Fr9Rb5npBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lst = sorted(zip(labels, pred), key=lambda x:-x[1])"
      ],
      "metadata": {
        "id": "wAcpWeyut9As"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = [y for x,y in lst]\n",
        "labels = [x for x,y in lst]\n",
        "labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WU1QXVqtu89A",
        "outputId": "f0794c6f-df58-4b3d-e961-7c29a470211d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['aspergers',\n",
              " 'OCPD',\n",
              " 'Schizoid',\n",
              " 'ARFID',\n",
              " 'DysmorphicDisorder',\n",
              " 'schizophrenia',\n",
              " 'Tourettes',\n",
              " 'depression1',\n",
              " 'ADHD',\n",
              " 'Anger',\n",
              " 'anxiety',\n",
              " 'MentalHealthSupport',\n",
              " 'dpdr',\n",
              " 'addiction',\n",
              " 'BorderlinePDisorder',\n",
              " 'selfharm',\n",
              " 'askscience']"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dAGaKcOut9Dc",
        "outputId": "162f6463-3db1-443d-85f9-c83e3468107c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([3.81853967e-10, 3.62916963e-05, 2.85033089e-10, 8.00661799e-18,\n",
              "        1.26523161e-07, 8.90305929e-14, 5.77042579e-02, 1.22176247e-03,\n",
              "        1.14104877e-09, 4.06144758e-15, 1.21280732e-10, 2.74370014e-44,\n",
              "        9.41037549e-01, 9.31560767e-10, 4.73358119e-14, 9.73188602e-09,\n",
              "        1.71139763e-23])]"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KL6opeJlbd5U",
        "outputId": "465dc58b-8a57-464b-a773-3479ff69a0fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classifying by chance would result in 5.88\\% accuracy.\n",
            "So comparing to that, the model is doing a good job?!\n"
          ]
        }
      ],
      "source": [
        "by_chance = 1/17 * 100\n",
        "print('classifying by chance would result in {:.2f}\\% accuracy.\\nSo comparing to that, the model is doing a good job?!'.format(by_chance))\n",
        "      "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JeWNJ7Zrbd5U"
      },
      "outputs": [],
      "source": [
        "clf = LogisticRegression()\n",
        "clf.fit(X_train_tr,y_train)\n",
        "y_pred = clf.predict(X_test_tr)\n",
        "print('Accuracy: {:.2f}%\\n'.format(100*accuracy_score(y_test, y_pred)))\n",
        "print('Precision (micro): {:.2f}%'.format(100*precision_score(y_test, y_pred, average='micro'))) \n",
        "print('Recall (micro): {:.2f}%'.format(100*recall_score(y_test, y_pred, average='micro')))  \n",
        "print('F1 (micro): {:.2f}%\\n'.format(100*f1_score(y_test, y_pred, average='micro'))) \n",
        "print('Precision (macro): {:.2f}%'.format(100*precision_score(y_test, y_pred, average='macro'))) \n",
        "print('Recall (macro): {:.2f}%'.format(100*recall_score(y_test, y_pred, average='macro'))) \n",
        "print('F1 (macro): {:.2f}%'.format(100*f1_score(y_test, y_pred, average='macro')))   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m9f4WisSbd5V"
      },
      "outputs": [],
      "source": [
        "clf = HistGradientBoostingClassifier()\n",
        "clf.fit(X_train_tr,y_train)\n",
        "y_pred = clf.predict(X_test_tr)\n",
        "print('Accuracy: {:.2f}%\\n'.format(100*accuracy_score(y_test, y_pred)))\n",
        "print('Precision (micro): {:.2f}%'.format(100*precision_score(y_test, y_pred, average='micro'))) \n",
        "print('Recall (micro): {:.2f}%'.format(100*recall_score(y_test, y_pred, average='micro')))  \n",
        "print('F1 (micro): {:.2f}%\\n'.format(100*f1_score(y_test, y_pred, average='micro'))) \n",
        "print('Precision (macro): {:.2f}%'.format(100*precision_score(y_test, y_pred, average='macro'))) \n",
        "print('Recall (macro): {:.2f}%'.format(100*recall_score(y_test, y_pred, average='macro'))) \n",
        "print('F1 (macro): {:.2f}%'.format(100*f1_score(y_test, y_pred, average='macro')))   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HwTqpTpRbd5V"
      },
      "outputs": [],
      "source": [
        "clf = MLPClassifier()\n",
        "clf.fit(X_train_tr,y_train)\n",
        "y_pred = clf.predict(X_test_tr)\n",
        "print('Accuracy: {:.2f}%\\n'.format(100*accuracy_score(y_test, y_pred)))\n",
        "print('Precision (micro): {:.2f}%'.format(100*precision_score(y_test, y_pred, average='micro'))) \n",
        "print('Recall (micro): {:.2f}%'.format(100*recall_score(y_test, y_pred, average='micro')))  \n",
        "print('F1 (micro): {:.2f}%\\n'.format(100*f1_score(y_test, y_pred, average='micro'))) \n",
        "print('Precision (macro): {:.2f}%'.format(100*precision_score(y_test, y_pred, average='macro'))) \n",
        "print('Recall (macro): {:.2f}%'.format(100*recall_score(y_test, y_pred, average='macro'))) \n",
        "print('F1 (macro): {:.2f}%'.format(100*f1_score(y_test, y_pred, average='macro')))   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AStEGoNmbd5V"
      },
      "outputs": [],
      "source": [
        "clf = RandomforestClassifier()\n",
        "clf.fit(X_train_tr,y_train)\n",
        "y_pred = clf.predict(X_test_tr)\n",
        "print('Accuracy: {:.2f}%\\n'.format(100*accuracy_score(y_test, y_pred)))\n",
        "print('Precision (micro): {:.2f}%'.format(100*precision_score(y_test, y_pred, average='micro'))) \n",
        "print('Recall (micro): {:.2f}%'.format(100*recall_score(y_test, y_pred, average='micro')))  \n",
        "print('F1 (micro): {:.2f}%\\n'.format(100*f1_score(y_test, y_pred, average='micro'))) \n",
        "print('Precision (macro): {:.2f}%'.format(100*precision_score(y_test, y_pred, average='macro'))) \n",
        "print('Recall (macro): {:.2f}%'.format(100*recall_score(y_test, y_pred, average='macro'))) \n",
        "print('F1 (macro): {:.2f}%'.format(100*f1_score(y_test, y_pred, average='macro')))   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r2IAjzXAbd5V"
      },
      "outputs": [],
      "source": [
        "clf = StackingClassifier(estimators=[('forest',  RandomforestClassifier()),\n",
        "                                     ('gboost',  HistGradientBoostingClassifier()),\n",
        "                                     ('MLP',     MLPClassifier()),\n",
        "                                     ('passAgg', PassiveAggressiveClassifier())],\n",
        "                         final_estimator=Ridge()\n",
        "\n",
        "clf.fit(X_train_tr,y_train)\n",
        "y_pred = clf.predict(X_test_tr)\n",
        "print('Accuracy: {:.2f}%\\n'.format(100*accuracy_score(y_test, y_pred)))\n",
        "print('Precision (micro): {:.2f}%'.format(100*precision_score(y_test, y_pred, average='micro'))) \n",
        "print('Recall (micro): {:.2f}%'.format(100*recall_score(y_test, y_pred, average='micro')))  \n",
        "print('F1 (micro): {:.2f}%\\n'.format(100*f1_score(y_test, y_pred, average='micro'))) \n",
        "print('Precision (macro): {:.2f}%'.format(100*precision_score(y_test, y_pred, average='macro'))) \n",
        "print('Recall (macro): {:.2f}%'.format(100*recall_score(y_test, y_pred, average='macro'))) \n",
        "print('F1 (macro): {:.2f}%'.format(100*f1_score(y_test, y_pred, average='macro')))   "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "AEhTtAojGH7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ixFVK0y3GH_K"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "name": "Classical-ML.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}