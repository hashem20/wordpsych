{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "EORBrpfgPCh-"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/hashem/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/hashem/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# !pip install joblib\n",
    "# !pip install threadpoolctl\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV, SGDClassifier\n",
    "from sklearn.linear_model import RidgeClassifier, RidgeClassifierCV, Perceptron, PassiveAggressiveClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, VotingClassifier, StackingClassifier, BaggingClassifier \n",
    "from sklearn.neighbors import KNeighborsClassifier, RadiusNeighborsClassifier, NearestCentroid\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB, CategoricalNB, ComplementNB\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.multioutput import ClassifierChain, MultiOutputClassifier\n",
    "from sklearn.semi_supervised import LabelPropagation, LabelSpreading, SelfTrainingClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, ClassifierMixin\n",
    "\n",
    "from collections import Counter\n",
    "from scipy.sparse import csr_matrix\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "\n",
    "# !pip install urlextract\n",
    "import urlextract\n",
    "find_urls = urlextract.URLExtract().find_urls\n",
    "\n",
    "# !pip install empath\n",
    "from empath import Empath\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "stem = nltk.PorterStemmer().stem\n",
    "\n",
    "path = '/Users/hashem/Python/TDI/capstone/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class get_text(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VZg7md1WNGHc"
   },
   "outputs": [],
   "source": [
    "# to merge two-part texts (text + selftext in submissions)\n",
    "def merge_text(t1, t2):\n",
    "    if isinstance(type(t1), str) and t1!='[removed]':\n",
    "        if isinstance(type(t2),str) and t2!='[removed]':\n",
    "            text = t1+' '+t2\n",
    "        else:\n",
    "            text = t1\n",
    "    elif isinstance(type(t2),str) and t2 != '[removed]':\n",
    "        text = t2\n",
    "    else:\n",
    "        text = ''\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "qNY_UYznOhvg"
   },
   "outputs": [],
   "source": [
    "# make texts lower-case\n",
    "class lower_case(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return [x.lower() for x in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "PKZ6SD-rOy0y"
   },
   "outputs": [],
   "source": [
    "# replace all urls with 'URL'\n",
    "class replace_urls(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = []\n",
    "        for text in X:\n",
    "            urls = list(set(find_urls(text)))\n",
    "            urls.sort(key=lambda x: len(x), reverse=True)\n",
    "            for url in urls:\n",
    "                text = text.replace(url, 'URL')\n",
    "            X_transformed.append(text)\n",
    "\n",
    "        return X_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "LR1Ps9Vbpkf0"
   },
   "outputs": [],
   "source": [
    "# replace all numbers with 'NUM'\n",
    "class replace_numbers(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = []\n",
    "        for text in X:\n",
    "            text_tr = re.sub(r'\\d+(?:\\.\\d*)?(?:[eE][+-]?\\d+)?', 'NUM', text)\n",
    "            X_transformed.append(text_tr)\n",
    "\n",
    "        return X_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "hXP4NSJQpsh1"
   },
   "outputs": [],
   "source": [
    "# limit size of the text\n",
    "class limit_size(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        index = [len(text) < self.size for text in X['text']]\n",
    "        return X[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "UPA5u6Fvpx7f"
   },
   "outputs": [],
   "source": [
    "# remove punctuations\n",
    "class remove_punctuation(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = []\n",
    "        for text in X:\n",
    "            text_tr = re.sub(r'\\W+', ' ', text, flags=re.M)\n",
    "            X_transformed.append(text_tr)\n",
    "\n",
    "        return X_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "2h2qOK_8p5s9"
   },
   "outputs": [],
   "source": [
    "# remove key words\n",
    "class remove_key_words(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, remove_list):\n",
    "        self.remove_list = remove_list\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = []\n",
    "        for text in X:\n",
    "            for word in self.remove_list:\n",
    "                text = text.replace(word, '')\n",
    "            X_transformed.append(text)\n",
    "\n",
    "        return X_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "EhLKBc5Gp_cJ"
   },
   "outputs": [],
   "source": [
    "# remove stopwords\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "class remove_stopwords(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, stopwords=stopwords):\n",
    "        self.stopwords = stopwords\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = []\n",
    "        for text in X:\n",
    "            text_=''\n",
    "            for word in nltk.tokenize.word_tokenize(text):\n",
    "                if word not in self.stopwords:\n",
    "                    text_ = ' '.join([text_,word])\n",
    "            X_transformed.append(text_)\n",
    "\n",
    "        return X_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "WRZFCE8xqF4h"
   },
   "outputs": [],
   "source": [
    "# counting the number of words in a text\n",
    "class wordCount(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, stemming=True):\n",
    "        self.stemming = stemming\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = []\n",
    "        for text in X:\n",
    "            word_counts = Counter(text.split())\n",
    "            \n",
    "            if self.stemming:\n",
    "                stemmed_word_counts = Counter()\n",
    "                for word, count in word_counts.items():\n",
    "                    stemmed_word = stem(word)\n",
    "                    stemmed_word_counts[stemmed_word] += count\n",
    "                word_counts = stemmed_word_counts\n",
    "                \n",
    "            X_transformed.append(word_counts)\n",
    "            \n",
    "        return X_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "6naQLr5aqWxa"
   },
   "outputs": [],
   "source": [
    "# transfering words to vectors\n",
    "class toVector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, vocabulary_size=10000):\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        total_count = Counter()\n",
    "        for word_count in X:\n",
    "            for word, count in word_count.items():\n",
    "                # total_count[word] += count\n",
    "                total_count[word] += min(count, 10)\n",
    "        most_common = total_count.most_common()[:self.vocabulary_size]\n",
    "        self.vocabulary_ = {word: index + 1 for index, (word, count) in enumerate(most_common)}\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        rows = []\n",
    "        cols = []\n",
    "        data = []\n",
    "        for row, word_count in enumerate(X):\n",
    "            for word, count in word_count.items():\n",
    "                rows.append(row)\n",
    "                cols.append(self.vocabulary_.get(word, 0))\n",
    "                data.append(count)\n",
    "        csr = csr_matrix((data, (rows, cols)), shape=(len(X), len(self.vocabulary_) + 1))\n",
    "        return csr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "NSBmcivLqh8U"
   },
   "outputs": [],
   "source": [
    "class empath(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        empath = []\n",
    "        for text in X:\n",
    "            empath.append(Empath().analyze(text, normalize=True))\n",
    "        return empath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = {'control':['AskReddit_1.csv', 'AskReddit2_1.csv'],\n",
    "          'ocd':['OCD_1.csv','OCPD_1.csv'],\n",
    "          'autism':['AutisticAdults_1.csv', 'autism_1.csv', 'asperger_1.csv', 'AutismInWomen_1.csv'],\n",
    "          'depression':['sad_1.csv', 'depression_1.csv'],\n",
    "          'adhd':['ADHD_1.csv', 'ADHD2_1.csv'],\n",
    "          'borderline':['Borderline_1.csv'],\n",
    "          'socialanxiety':['socialanxiety2_1.csv', 'socialanxiety_1.csv'],\n",
    "          'phobia':['emetophobia_1.csv', 'Agoraphobia_1.csv', 'Phobia_1.csv'],\n",
    "          'general':['mentalillness_1.csv', 'personalitydisorders_1.csv', 'mental_1.csv', 'MentalHealthPH_1.csv'],\n",
    "          'avoidant':['Avoidant_1.csv'],\n",
    "          'aspd':['aspd_1.csv','psychopath_1.csv'],\n",
    "          'narcissism':['narcissism_1.csv'],\n",
    "          'schizophrenia':['schizoaffective_1.csv', 'paranoidschizophrenia_1.csv', 'Psychosis_1.csv', \n",
    "                           'schizophrenia_1.csv', 'Paranoid_1.csv'],\n",
    "          'anger':['Anger_1.csv'],\n",
    "          'body':['DysmorphicDisorder_1.csv', 'BodyAcceptance_1.csv'],\n",
    "          'addiction':['addiction_1.csv', 'alcoholism_1.csv'],\n",
    "          'schizoid':['Schizoid_1.csv', 'Schizotypal_1.csv'],\n",
    "          'eating':['EatingDisorders_1.csv', 'bulimia_1.csv'],\n",
    "          'bipolar':['bipolar_1.csv'],\n",
    "          'ptsd':['ptsd_1.csv', 'CPTSD_1.csv'],\n",
    "          'selfhard':['selfharm_1.csv', 'SuicideWatch_1.csv']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Zj4RuDaVAPGA"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tourettes_1.csv',\n",
       " 'AskReddit_1.csv',\n",
       " 'OCD_1.csv',\n",
       " 'emetophobia_1.csv',\n",
       " 'AutisticAdults_1.csv',\n",
       " 'mentalillness_1.csv',\n",
       " 'autism_1.csv',\n",
       " 'sad_1.csv',\n",
       " 'BodyAcceptance_1.csv',\n",
       " 'ADHD_1.csv',\n",
       " 'Borderline_1.csv',\n",
       " 'Avoidant_1.csv',\n",
       " 'asperger_1.csv',\n",
       " 'aspd_1.csv',\n",
       " 'hpd_1.csv',\n",
       " 'OCPD_1.csv',\n",
       " 'ADHD2_1.csv',\n",
       " 'narcissism_1.csv',\n",
       " 'Anger_1.csv',\n",
       " 'selfharm_1.csv',\n",
       " 'schizoaffective_1.csv',\n",
       " 'Agoraphobia_1.csv',\n",
       " 'Psychopathy_1.csv',\n",
       " 'dpdr_1.csv',\n",
       " 'depression_1.csv',\n",
       " 'AutismInWomen_1.csv',\n",
       " 'Paranoid_1.csv',\n",
       " 'ptsd_1.csv',\n",
       " 'schizophrenia_1.csv',\n",
       " 'MentalHealthPH_1.csv',\n",
       " 'SuicideWatch_1.csv',\n",
       " 'alcoholism_1.csv',\n",
       " 'Schizotypal_1.csv',\n",
       " 'bulimia_1.csv',\n",
       " 'bipolar_1.csv',\n",
       " 'socialanxiety_1.csv',\n",
       " 'socialanxiety2_1.csv',\n",
       " 'EatingDisorders_1.csv',\n",
       " 'psychopath_1.csv',\n",
       " 'mental_1.csv',\n",
       " 'Psychosis_1.csv',\n",
       " 'Schizoid_1.csv',\n",
       " 'personalitydisorders_1.csv',\n",
       " 'Phobia_1.csv',\n",
       " 'AskReddit2_1.csv',\n",
       " 'CPTSD_1.csv',\n",
       " 'addiction_1.csv',\n",
       " 'DysmorphicDisorder_1.csv',\n",
       " 'paranoidschizophrenia_1.csv']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "f45cYID43SjN"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get X, y\n",
    "X=[]\n",
    "y=[]\n",
    "for key,gs in groups.items():\n",
    "    for g in gs:\n",
    "        df = pd.read_csv('data/'+g, usecols=['text'], lineterminator='\\n')\n",
    "        X.extend(df.text)\n",
    "        y.extend([key]*len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(x,y) for x,y in zip(X,y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample size:  1161324\n",
      "Accuracy: 20.35%\n",
      "\n",
      "Precision (micro): 20.35%\n",
      "Recall (micro): 20.35%\n",
      "F1 (micro): 20.35%\n",
      "\n",
      "Precision (macro): 50.03%\n",
      "Recall (macro): 13.29%\n",
      "F1 (macro): 14.23%\n"
     ]
    }
   ],
   "source": [
    "limit = 200\n",
    "\n",
    "data_limited = [(x,y) for x,y in data if len(x)>limit]\n",
    "X_limited = [x for x,y in data_limited]\n",
    "y_limited = [y for x,y in data_limited]\n",
    "\n",
    "vec = TfidfVectorizer(min_df=10)\n",
    "X_tr = vec.fit_transform(X_limited)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tr,y_limited)\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print('sample size: ', len(y_limited))\n",
    "print('Accuracy: {:.2f}%\\n'.format(100*accuracy_score(y_test, y_pred)))\n",
    "print('Precision (micro): {:.2f}%'.format(100*precision_score(y_test, y_pred, average='micro'))) \n",
    "print('Recall (micro): {:.2f}%'.format(100*recall_score(y_test, y_pred, average='micro')))  \n",
    "print('F1 (micro): {:.2f}%\\n'.format(100*f1_score(y_test, y_pred, average='micro'))) \n",
    "print('Precision (macro): {:.2f}%'.format(100*precision_score(y_test, y_pred, average='macro'))) \n",
    "print('Recall (macro): {:.2f}%'.format(100*recall_score(y_test, y_pred, average='macro'))) \n",
    "print('F1 (macro): {:.2f}%'.format(100*f1_score(y_test, y_pred, average='macro')))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "import re\n",
    "\n",
    "def process_review(review):\n",
    "    \"\"\"\n",
    "    Splits review into sentences, then sentences into tokens. Returns \n",
    "    nested list.\n",
    "    \"\"\"\n",
    "    words = [simple_preprocess(sentence, deacc=True) \n",
    "             for sentence in re.split('\\.|\\?|\\!', review)\n",
    "             if sentence]\n",
    "    return words\n",
    "\n",
    "limit = 200\n",
    "\n",
    "data_limited = [(x,y) for x,y in data if len(x)>limit]\n",
    "X_limited = [x for x,y in data_limited]\n",
    "y_limited = [y for x,y in data_limited]\n",
    "\n",
    "# Flatten list to contain all sentences from all reviews\n",
    "sentences = [sentence for review in X_limited \n",
    "             for sentence in process_review(review)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8330686"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_sizes = [len(x.split()) for x in X_limited]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.157667e+06, 3.126000e+03, 3.680000e+02, 9.700000e+01,\n",
       "        3.100000e+01, 2.000000e+01, 9.000000e+00, 4.000000e+00,\n",
       "        0.000000e+00, 2.000000e+00]),\n",
       " array([1.0000e+00, 9.1590e+02, 1.8308e+03, 2.7457e+03, 3.6606e+03,\n",
       "        4.5755e+03, 5.4904e+03, 6.4053e+03, 7.3202e+03, 8.2351e+03,\n",
       "        9.1500e+03]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(token_sizes, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import gaussian_kde\n",
    "kde = gaussian_kde(token_sizes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.arange(0,1000)\n",
    "y= kde.evaluate(x)\n",
    "plt.plot(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kde.evaluate(token_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = Word2Vec(sentences,\n",
    "               workers=3,   # Worker threads (=faster w/ multicore)\n",
    "               vector_size=400,    # Dimensionality of word vectors\n",
    "               window=11,    # Window size\n",
    "               min_count=10, # Ignore words with frequency lower than this\n",
    "               sample=1e-3) # Threshold for which higher-frequency \n",
    "                            # words are randomly downsampled\n",
    "\n",
    "w2v.save('w2v.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit=200\n",
    "data_limited = [(x,y) for x,y in data if len(x)>limit]\n",
    "X_limited = [x for x,y in data_limited]\n",
    "y_limited = [y for x,y in data_limited]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "w2v = Word2Vec.load('w2v.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(data, maxlen=500, embedding_dim=400):\n",
    "    \"\"\"\n",
    "    Tokenizes reviews, truncates the number of tokens if more than `maxlen`, \n",
    "    and vectorizes each token. Returns a three-dimensional array of shape\n",
    "    n reviews x `maxlen` x `embedding_dim`. \n",
    "    \"\"\"\n",
    "    # Create empty array\n",
    "    vectorized_data = np.zeros(shape=(len(data), maxlen, embedding_dim))\n",
    "    \n",
    "    for row, case in enumerate(data):\n",
    "        # Preprocess each review\n",
    "        tokens = simple_preprocess(case)\n",
    "        \n",
    "        # Truncate long reviews\n",
    "        if len(tokens) > maxlen:\n",
    "            tokens = tokens[:maxlen]\n",
    "        \n",
    "        # Get vector for each token in review\n",
    "        for col, token in enumerate(tokens):\n",
    "            try:\n",
    "                word_vector = w2v.wv[token]\n",
    "                # Add vector to array\n",
    "                vectorized_data[row, col] = word_vector[:embedding_dim]\n",
    "            except KeyError:\n",
    "                pass\n",
    "    \n",
    "    return vectorized_data\n",
    "\n",
    "maxlen = 500        # Our predetermined limit\n",
    "embedding_dim = 400 # The first 100 values in our w2v vectors\n",
    "\n",
    "X = vectorize(X_limited, maxlen, embedding_dim)\n",
    "y = np.array(y_limited).reshape(-1,1)\n",
    "\n",
    "print('Shape of feature matrix:', X.shape)\n",
    "print('Shape of target vector:', y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import GRU, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "            \n",
    "gru_model = Sequential([\n",
    "    GRU(128, return_sequences=True, input_shape=(maxlen, embedding_dim)),\n",
    "    Dropout(0.25),\n",
    "    GRU(64),\n",
    "    Dropout(0.25),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "print(gru_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('lower', lower_case()),\n",
    "                 ('url', replace_urls()),\n",
    "                 ('num', replace_numbers()),\n",
    "                 ('punc', remove_punctuation()),\n",
    "                 ('stop', remove_stopwords()),\n",
    "                 ('count', wordCount()),\n",
    "                 ('vec', toVector())])\n",
    "X_tr = pipe.fit_transform(X_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('transformed_bag.pkl', 'wb') as f:\n",
    "    pickle.dump(X_tr, f)\n",
    "    pickle.dump(y_100, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('transformed_bag.pkl', 'rb') as f:\n",
    "    X_tr_ = pickle.load(f)\n",
    "    y_100_ = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2286592, (2286592, 10001))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_100_), X_tr_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_tr_,y_100_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1714944, 10001), (571648, 10001), 1714944, 571648)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, len(y_train), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 15.23%\n",
      "\n",
      "Precision (micro): 15.23%\n",
      "Recall (micro): 15.23%\n",
      "F1 (micro): 15.23%\n",
      "\n",
      "Precision (macro): 14.55%\n",
      "Recall (macro): 13.98%\n",
      "F1 (macro): 13.70%\n"
     ]
    }
   ],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print('Accuracy: {:.2f}%\\n'.format(100*accuracy_score(y_test, y_pred)))\n",
    "print('Precision (micro): {:.2f}%'.format(100*precision_score(y_test, y_pred, average='micro'))) \n",
    "print('Recall (micro): {:.2f}%'.format(100*recall_score(y_test, y_pred, average='micro')))  \n",
    "print('F1 (micro): {:.2f}%\\n'.format(100*f1_score(y_test, y_pred, average='micro'))) \n",
    "print('Precision (macro): {:.2f}%'.format(100*precision_score(y_test, y_pred, average='macro'))) \n",
    "print('Recall (macro): {:.2f}%'.format(100*recall_score(y_test, y_pred, average='macro'))) \n",
    "print('F1 (macro): {:.2f}%'.format(100*f1_score(y_test, y_pred, average='macro')))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 19.15%\n",
      "\n",
      "Precision (micro): 19.15%\n",
      "Recall (micro): 19.15%\n",
      "F1 (micro): 19.15%\n",
      "\n",
      "Precision (macro): 31.28%\n",
      "Recall (macro): 13.69%\n",
      "F1 (macro): 14.52%\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: {:.2f}%\\n'.format(100*accuracy_score(y_test, y_pred)))\n",
    "print('Precision (micro): {:.2f}%'.format(100*precision_score(y_test, y_pred, average='micro'))) \n",
    "print('Recall (micro): {:.2f}%'.format(100*recall_score(y_test, y_pred, average='micro')))  \n",
    "print('F1 (micro): {:.2f}%\\n'.format(100*f1_score(y_test, y_pred, average='micro'))) \n",
    "print('Precision (macro): {:.2f}%'.format(100*precision_score(y_test, y_pred, average='macro'))) \n",
    "print('Recall (macro): {:.2f}%'.format(100*recall_score(y_test, y_pred, average='macro'))) \n",
    "print('F1 (macro): {:.2f}%'.format(100*f1_score(y_test, y_pred, average='macro')))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifying by chance would result in 4.76\\% accuracy.\n",
      "So comparing to that, the model is doing a good job?!\n"
     ]
    }
   ],
   "source": [
    "by_chance = 1/len(groups) * 100\n",
    "print('classifying by chance would result in {:.2f}\\% accuracy.\\nSo comparing to that, the model is doing a good job?!'.format(by_chance))\n",
    "      \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(X_train_tr,y_train)\n",
    "y_pred = clf.predict(X_test_tr)\n",
    "print('Accuracy: {:.2f}%\\n'.format(100*accuracy_score(y_test, y_pred)))\n",
    "print('Precision (micro): {:.2f}%'.format(100*precision_score(y_test, y_pred, average='micro'))) \n",
    "print('Recall (micro): {:.2f}%'.format(100*recall_score(y_test, y_pred, average='micro')))  \n",
    "print('F1 (micro): {:.2f}%\\n'.format(100*f1_score(y_test, y_pred, average='micro'))) \n",
    "print('Precision (macro): {:.2f}%'.format(100*precision_score(y_test, y_pred, average='macro'))) \n",
    "print('Recall (macro): {:.2f}%'.format(100*recall_score(y_test, y_pred, average='macro'))) \n",
    "print('F1 (macro): {:.2f}%'.format(100*f1_score(y_test, y_pred, average='macro')))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [37]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m clf \u001b[38;5;241m=\u001b[39m GradientBoostingClassifier()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mpredict(X_test_tr)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccuracy: \u001b[39m\u001b[38;5;132;01m{:.2f}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;241m100\u001b[39m\u001b[38;5;241m*\u001b[39maccuracy_score(y_test, y_pred)))\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.9/site-packages/sklearn/ensemble/_gb.py:586\u001b[0m, in \u001b[0;36mBaseGradientBoosting.fit\u001b[0;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resize_state()\n\u001b[1;32m    585\u001b[0m \u001b[38;5;66;03m# fit the boosting stages\u001b[39;00m\n\u001b[0;32m--> 586\u001b[0m n_stages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_stages\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rng\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbegin_at_stage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;66;03m# change shape of arrays after fit (early-stopping or additional ests)\u001b[39;00m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_stages \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.9/site-packages/sklearn/ensemble/_gb.py:663\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stages\u001b[0;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor)\u001b[0m\n\u001b[1;32m    656\u001b[0m     old_oob_score \u001b[38;5;241m=\u001b[39m loss_(\n\u001b[1;32m    657\u001b[0m         y[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[1;32m    658\u001b[0m         raw_predictions[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[1;32m    659\u001b[0m         sample_weight[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[1;32m    660\u001b[0m     )\n\u001b[1;32m    662\u001b[0m \u001b[38;5;66;03m# fit next stage of trees\u001b[39;00m\n\u001b[0;32m--> 663\u001b[0m raw_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_stage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_csc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_csr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;66;03m# track deviance (= loss)\u001b[39;00m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_oob:\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.9/site-packages/sklearn/ensemble/_gb.py:222\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stage\u001b[0;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_csc, X_csr)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mis_multi_class:\n\u001b[1;32m    220\u001b[0m     y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(original_y \u001b[38;5;241m==\u001b[39m k, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[0;32m--> 222\u001b[0m residual \u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnegative_gradient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw_predictions_copy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;66;03m# induce regression tree on residuals\u001b[39;00m\n\u001b[1;32m    227\u001b[0m tree \u001b[38;5;241m=\u001b[39m DecisionTreeRegressor(\n\u001b[1;32m    228\u001b[0m     criterion\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion,\n\u001b[1;32m    229\u001b[0m     splitter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    238\u001b[0m     ccp_alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mccp_alpha,\n\u001b[1;32m    239\u001b[0m )\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.9/site-packages/sklearn/ensemble/_gb_losses.py:823\u001b[0m, in \u001b[0;36mMultinomialDeviance.negative_gradient\u001b[0;34m(self, y, raw_predictions, k, **kwargs)\u001b[0m\n\u001b[1;32m    807\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnegative_gradient\u001b[39m(\u001b[38;5;28mself\u001b[39m, y, raw_predictions, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    808\u001b[0m     \u001b[38;5;124;03m\"\"\"Compute negative gradient for the ``k``-th class.\u001b[39;00m\n\u001b[1;32m    809\u001b[0m \n\u001b[1;32m    810\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m \u001b[38;5;124;03m        The index of the class.\u001b[39;00m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    822\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m y \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mnan_to_num(\n\u001b[0;32m--> 823\u001b[0m         np\u001b[38;5;241m.\u001b[39mexp(raw_predictions[:, k] \u001b[38;5;241m-\u001b[39m \u001b[43mlogsumexp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    824\u001b[0m     )\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.9/site-packages/scipy/special/_logsumexp.py:110\u001b[0m, in \u001b[0;36mlogsumexp\u001b[0;34m(a, axis, b, keepdims, return_sign)\u001b[0m\n\u001b[1;32m    108\u001b[0m     tmp \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mexp(a \u001b[38;5;241m-\u001b[39m a_max)\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 110\u001b[0m     tmp \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexp(\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43ma_max\u001b[49m)\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# suppress warnings about log of zero\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(divide\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "clf = GradientBoostingClassifier()\n",
    "clf.fit(X_train_tr,y_train)\n",
    "y_pred = clf.predict(X_test_tr)\n",
    "print('Accuracy: {:.2f}%\\n'.format(100*accuracy_score(y_test, y_pred)))\n",
    "print('Precision (micro): {:.2f}%'.format(100*precision_score(y_test, y_pred, average='micro'))) \n",
    "print('Recall (micro): {:.2f}%'.format(100*recall_score(y_test, y_pred, average='micro')))  \n",
    "print('F1 (micro): {:.2f}%\\n'.format(100*f1_score(y_test, y_pred, average='micro'))) \n",
    "print('Precision (macro): {:.2f}%'.format(100*precision_score(y_test, y_pred, average='macro'))) \n",
    "print('Recall (macro): {:.2f}%'.format(100*recall_score(y_test, y_pred, average='macro'))) \n",
    "print('F1 (macro): {:.2f}%'.format(100*f1_score(y_test, y_pred, average='macro')))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier()\n",
    "clf.fit(X_train_tr,y_train)\n",
    "y_pred = clf.predict(X_test_tr)\n",
    "print('Accuracy: {:.2f}%\\n'.format(100*accuracy_score(y_test, y_pred)))\n",
    "print('Precision (micro): {:.2f}%'.format(100*precision_score(y_test, y_pred, average='micro'))) \n",
    "print('Recall (micro): {:.2f}%'.format(100*recall_score(y_test, y_pred, average='micro')))  \n",
    "print('F1 (micro): {:.2f}%\\n'.format(100*f1_score(y_test, y_pred, average='micro'))) \n",
    "print('Precision (macro): {:.2f}%'.format(100*precision_score(y_test, y_pred, average='macro'))) \n",
    "print('Recall (macro): {:.2f}%'.format(100*recall_score(y_test, y_pred, average='macro'))) \n",
    "print('F1 (macro): {:.2f}%'.format(100*f1_score(y_test, y_pred, average='macro')))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomforestClassifier()\n",
    "clf.fit(X_train_tr,y_train)\n",
    "y_pred = clf.predict(X_test_tr)\n",
    "print('Accuracy: {:.2f}%\\n'.format(100*accuracy_score(y_test, y_pred)))\n",
    "print('Precision (micro): {:.2f}%'.format(100*precision_score(y_test, y_pred, average='micro'))) \n",
    "print('Recall (micro): {:.2f}%'.format(100*recall_score(y_test, y_pred, average='micro')))  \n",
    "print('F1 (micro): {:.2f}%\\n'.format(100*f1_score(y_test, y_pred, average='micro'))) \n",
    "print('Precision (macro): {:.2f}%'.format(100*precision_score(y_test, y_pred, average='macro'))) \n",
    "print('Recall (macro): {:.2f}%'.format(100*recall_score(y_test, y_pred, average='macro'))) \n",
    "print('F1 (macro): {:.2f}%'.format(100*f1_score(y_test, y_pred, average='macro')))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = StackingClassifier(estimators=[('forest',  RandomforestClassifier()),\n",
    "                                     ('gboost',  HistGradientBoostingClassifier()),\n",
    "                                     ('MLP',     MLPClassifier()),\n",
    "                                     ('passAgg', PassiveAggressiveClassifier())],\n",
    "                         final_estimator=Ridge()\n",
    "\n",
    "clf.fit(X_train_tr,y_train)\n",
    "y_pred = clf.predict(X_test_tr)\n",
    "print('Accuracy: {:.2f}%\\n'.format(100*accuracy_score(y_test, y_pred)))\n",
    "print('Precision (micro): {:.2f}%'.format(100*precision_score(y_test, y_pred, average='micro'))) \n",
    "print('Recall (micro): {:.2f}%'.format(100*recall_score(y_test, y_pred, average='micro')))  \n",
    "print('F1 (micro): {:.2f}%\\n'.format(100*f1_score(y_test, y_pred, average='micro'))) \n",
    "print('Precision (macro): {:.2f}%'.format(100*precision_score(y_test, y_pred, average='macro'))) \n",
    "print('Recall (macro): {:.2f}%'.format(100*recall_score(y_test, y_pred, average='macro'))) \n",
    "print('F1 (macro): {:.2f}%'.format(100*f1_score(y_test, y_pred, average='macro')))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOvtaqTaJaaVwF9qw8mpPz6",
   "machine_shape": "hm",
   "mount_file_id": "17Ru6vIFn8-RdK1CYB1zSgjyE61-FtNoD",
   "name": "Classical-ML.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
